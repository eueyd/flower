# model_components.py
"""
æ¨¡å‹ç»„ä»¶å®šä¹‰
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models
from config import NUM_CLASSES, NUM_REGIONS


class RegionProposalNetwork(nn.Module):
    """åŒºåŸŸå»ºè®®ç½‘ç»œ"""

    def __init__(self, in_channels, hidden_dim=256, num_regions=2):
        super().__init__()
        self.num_regions = num_regions

        self.conv1 = nn.Conv2d(in_channels, hidden_dim, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(hidden_dim)
        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(hidden_dim)
        self.conv3 = nn.Conv2d(hidden_dim, num_regions, 1)

        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = self.conv3(x)
        attention_maps = self.softmax(x)
        return attention_maps


class FeaturePyramidFusion(nn.Module):
    """ç‰¹å¾é‡‘å­—å¡”èåˆæ¨¡å—"""

    def __init__(self, channels_list, output_dim=256):
        super().__init__()
        self.convs = nn.ModuleList()

        for channels in channels_list:
            self.convs.append(nn.Conv2d(channels, output_dim, 1))

        self.fusion_conv = nn.Sequential(
            nn.Conv2d(output_dim * len(channels_list), output_dim, 3, padding=1),
            nn.BatchNorm2d(output_dim),
            nn.ReLU(inplace=True),
            nn.Conv2d(output_dim, output_dim, 3, padding=1),
            nn.BatchNorm2d(output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, *feats):
        target_size = feats[0].shape[2:]
        processed_feats = []

        for i, (conv, feat) in enumerate(zip(self.convs, feats)):
            feat = conv(feat)
            if feat.shape[2:] != target_size:
                feat = F.interpolate(feat, size=target_size,
                                     mode='bilinear', align_corners=False)
            processed_feats.append(feat)

        fused = torch.cat(processed_feats, dim=1)
        fused = self.fusion_conv(fused)
        return fused


class RegionFeatureEnhancer(nn.Module):
    """åŒºåŸŸç‰¹å¾å¢å¼ºæ¨¡å—"""

    def __init__(self, global_dim, region_dim, output_dim):
        super().__init__()
        self.global_proj = nn.Linear(global_dim, region_dim)
        self.region_proj = nn.Linear(region_dim, region_dim)

        self.enhance = nn.Sequential(
            nn.Linear(global_dim + region_dim, output_dim * 2),
            nn.ReLU(inplace=True),
            nn.Linear(output_dim * 2, output_dim),
            nn.LayerNorm(output_dim),
            nn.ReLU(inplace=True)
        )

    def forward(self, global_feat, region_feat):
        global_proj = self.global_proj(global_feat)
        region_proj = self.region_proj(region_feat)

        attention = torch.sigmoid(torch.sum(global_proj * region_proj, dim=1, keepdim=True))
        enhanced = region_feat * attention

        combined = torch.cat([global_feat, enhanced], dim=1)
        output = self.enhance(combined)
        return output


class AdaptiveFusionModule(nn.Module):
    """è‡ªé€‚åº”ç‰¹å¾èåˆæ¨¡å—"""

    def __init__(self, global_dim, region_dim, num_regions, hidden_dim=256):
        super().__init__()
        self.num_regions = num_regions
        self.global_dim = global_dim
        self.region_dim = region_dim
        self.input_dim = region_dim * (1 + num_regions)  # 192

        self.global_proj = nn.Linear(global_dim, region_dim)

        # æ³¨æ„åŠ›ç½‘ç»œ
        self.attention = nn.Sequential(
            nn.Linear(self.input_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 1 + num_regions),
            nn.Softmax(dim=1)
        )

        # åŠ æƒç‰¹å¾æŠ•å½±å±‚ - æ–°å¢ï¼
        self.weighted_expansion = nn.Linear(region_dim, self.input_dim)

        # èåˆç½‘ç»œ
        self.fusion = nn.Sequential(
            nn.Linear(self.input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, 128)
        )
        self.gate_proj = nn.Linear(region_dim, self.input_dim)
        self.sigmoid = nn.Sigmoid()

    def forward(self, global_feat, region_feats):
        global_feat_proj = self.global_proj(global_feat)
        all_feats = [global_feat_proj] + region_feats
        concatenated = torch.cat(all_feats, dim=1)

        weights = self.attention(concatenated)

        # åŠ æƒèåˆ
        weighted_sum = weights[:, 0:1] * global_feat_proj
        for i in range(self.num_regions):
            weighted_sum = weighted_sum + weights[:, i + 1:i + 2] * region_feats[i]

        # æ–¹æ³•2ï¼šçº¿æ€§æŠ•å½±åŠ æƒç‰¹å¾ï¼Œç„¶åå¢å¼ºåŸå§‹ç‰¹å¾
        expanded_weighted = self.weighted_expansion(weighted_sum)
        gate = self.sigmoid(self.gate_proj(weighted_sum))  # ç”Ÿæˆé—¨æ§ä¿¡å·

        # å¢å¼ºåŸå§‹ç‰¹å¾ï¼šåŠ æƒç‰¹å¾ä½œä¸ºåç½®åŠ åˆ°åŸå§‹ç‰¹å¾ä¸Š
        enhanced = concatenated * gate + expanded_weighted * (1 - gate)  # é—¨æ§èåˆ

        fused_feature = self.fusion(enhanced)

        return fused_feature, weights

class ImprovedDualPathModel(nn.Module):
    """æ”¹è¿›çš„åŒè·¯å¾„æ¨¡å‹"""

    def __init__(self, num_classes=NUM_CLASSES, num_regions=NUM_REGIONS,
                 backbone_name='resnet18'):
        super().__init__()
        self.num_regions = num_regions
        self.num_classes = num_classes

        # éª¨å¹²ç½‘ç»œ
        if backbone_name == 'resnet18':
            backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)
            layer_channels = [64, 128, 256, 512]
        elif backbone_name == 'resnet34':
            backbone = models.resnet34(weights=models.ResNet34_Weights.IMAGENET1K_V1)
            layer_channels = [64, 128, 256, 512]
        elif backbone_name == 'resnet50':
            backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)
            layer_channels = [256, 512, 1024, 2048]
        else:
            raise ValueError(f"Unsupported backbone: {backbone_name}")

        self.conv1 = backbone.conv1
        self.bn1 = backbone.bn1
        self.relu = backbone.relu
        self.maxpool = backbone.maxpool
        self.layer1 = backbone.layer1
        self.layer2 = backbone.layer2
        self.layer3 = backbone.layer3
        self.layer4 = backbone.layer4

        # åŒºåŸŸå»ºè®®ç½‘ç»œ
        self.region_proposal = RegionProposalNetwork(
            in_channels=layer_channels[1],
            hidden_dim=128,
            num_regions=num_regions
        )

        # å¤šå°ºåº¦ç‰¹å¾èåˆ
        self.feature_fusion = FeaturePyramidFusion(
            channels_list=layer_channels[1:],
            output_dim=128
        )

        # åŒºåŸŸç‰¹å¾å¢å¼º
        self.region_feature_enhancer = nn.ModuleList([
            RegionFeatureEnhancer(
                global_dim=layer_channels[-1],
                region_dim=128,
                output_dim=64
            ) for _ in range(num_regions)
        ])

        # è‡ªé€‚åº”èåˆæ¨¡å—
        self.adaptive_fusion = AdaptiveFusionModule(
            global_dim=layer_channels[-1],
            region_dim=64,
            num_regions=num_regions,
            hidden_dim=128
        )

        # åˆ†ç±»å™¨
        self.global_classifier = nn.Sequential(
            nn.Linear(layer_channels[-1], 256),
            nn.LayerNorm(256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, 128),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(128, num_classes)
        )

        self.final_classifier = nn.Sequential(
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(64, 32),
            nn.ReLU(inplace=True),
            nn.Dropout(0.2),
            nn.Linear(32, num_classes)
        )

        # åŒºåŸŸåˆ†ç±»å™¨ï¼ˆè¾…åŠ©ç›‘ç£ï¼‰
        self.region_classifiers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(64, 32),
                nn.ReLU(inplace=True),
                nn.Linear(32, num_classes)
            ) for _ in range(num_regions)
        ])

        # åˆå§‹åŒ–
        self._initialize_weights()
        self._verify_trainability()

    def _initialize_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.normal_(m.weight, 0, 0.01)
                nn.init.constant_(m.bias, 0)

    def extract_features(self, x):
        """æå–å¤šå°ºåº¦ç‰¹å¾"""
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        feat1 = self.layer1(x)
        feat2 = self.layer2(feat1)
        feat3 = self.layer3(feat2)
        feat4 = self.layer4(feat3)

        return feat1, feat2, feat3, feat4

    def forward(self, x, return_attention=False):
        batch_size = x.size(0)

        # ç‰¹å¾æå–
        feat1, feat2, feat3, feat4 = self.extract_features(x)

        # å…¨å±€ç‰¹å¾
        global_pool = nn.AdaptiveAvgPool2d(1)
        global_feat = global_pool(feat4).view(batch_size, -1)

        # åŒºåŸŸå»ºè®®
        attention_maps = self.region_proposal(feat2)

        attention_maps_full = F.interpolate(
            attention_maps, size=x.shape[2:],
            mode='bilinear', align_corners=False
        )

        # å¤šå°ºåº¦ç‰¹å¾èåˆ
        fused_feat = self.feature_fusion(feat2, feat3, feat4)

        # åŒºåŸŸç‰¹å¾æå–ä¸å¢å¼º
        region_features = []
        region_logits = []

        for i in range(self.num_regions):
            region_mask = F.interpolate(
                attention_maps[:, i:i + 1],
                size=fused_feat.shape[2:],
                mode='bilinear', align_corners=False
            )

            region_feat_map = fused_feat * region_mask
            region_pool = nn.AdaptiveAvgPool2d(1)
            region_feat = region_pool(region_feat_map).view(batch_size, -1)

            enhanced_region_feat = self.region_feature_enhancer[i](global_feat, region_feat)
            region_features.append(enhanced_region_feat)

            region_logit = self.region_classifiers[i](enhanced_region_feat)
            region_logits.append(region_logit)

        # è‡ªé€‚åº”ç‰¹å¾èåˆ
        final_feature, fusion_weights = self.adaptive_fusion(global_feat, region_features)

        # åˆ†ç±»è¾“å‡º
        global_logits = self.global_classifier(global_feat)
        final_logits = self.final_classifier(final_feature)

        outputs = {
            'final_logits': final_logits,
            'global_logits': global_logits,
            'region_logits': torch.stack(region_logits, dim=1) if region_logits else None,
            'fused_feature': final_feature,
            'fusion_weights': fusion_weights,
            'attention_maps': attention_maps_full,
            'global_feature': global_feat
        }

        if return_attention:
            return outputs, attention_maps_full
        return outputs

    def compute_loss(self, outputs, labels, criterion):
        """è®¡ç®—å¤šä»»åŠ¡æŸå¤±"""
        from config import LAMBDA_GLOBAL, LAMBDA_REGION

        loss_final = criterion(outputs['final_logits'], labels)
        loss_global = criterion(outputs['global_logits'], labels)

        # åŒºåŸŸåˆ†ç±»æŸå¤±
        loss_region = 0
        if outputs['region_logits'] is not None:
            for i in range(self.num_regions):
                loss_region += criterion(outputs['region_logits'][:, i], labels)
            loss_region /= self.num_regions

        total_loss = (loss_final +
                      LAMBDA_GLOBAL * loss_global +
                      LAMBDA_REGION * loss_region)

        loss_details = {
            'loss_final': loss_final.item(),
            'loss_global': loss_global.item(),
            'loss_region': loss_region.item() if outputs['region_logits'] is not None else 0,
            'total_loss': total_loss.item()
        }

        return total_loss, loss_details

    def _verify_trainability(self):
        """éªŒè¯æ‰€æœ‰æ¨¡å—æ˜¯å¦å¯è®­ç»ƒ"""
        print(f"\\nğŸ” è®­ç»ƒæ€§æ£€æŸ¥:")
        for name, param in self.named_parameters():
            if not param.requires_grad:
                print(f"  âš ï¸ {name}: è¢«å†»ç»“ (requires_grad=False)")