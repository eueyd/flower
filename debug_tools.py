# debug_tools.py
"""
è°ƒè¯•å·¥å…·æ¨¡å—
"""

import torch
import torch.nn as nn
import numpy as np


class ModelDebugger:
    """æ¨¡å‹è°ƒè¯•å·¥å…·"""

    @staticmethod
    def check_module_output(module, input_tensor, module_name):
        """æ£€æŸ¥æ¨¡å—è¾“å‡º"""
        print(f"\nğŸ” æ£€æŸ¥ {module_name}:")
        print(f"  è¾“å…¥å½¢çŠ¶: {input_tensor.shape}")

        try:
            with torch.no_grad():
                output = module(input_tensor)

            print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")

            if torch.is_tensor(output):
                print(f"  æ•°å€¼èŒƒå›´: [{output.min():.6f}, {output.max():.6f}]")
                print(f"  å‡å€¼: {output.mean():.6f}, æ ‡å‡†å·®: {output.std():.6f}")

                if torch.isnan(output).any():
                    print("  âš ï¸ è­¦å‘Š: è¾“å‡ºåŒ…å«NaN!")
                if torch.isinf(output).any():
                    print("  âš ï¸ è­¦å‘Š: è¾“å‡ºåŒ…å«Inf!")

            return output

        except Exception as e:
            print(f"  âŒ é”™è¯¯: {e}")
            return None

    @staticmethod
    def check_gradient(model, loss):
        """æ£€æŸ¥æ¢¯åº¦"""
        print(f"\nğŸ” æ£€æŸ¥æ¢¯åº¦:")

        total_grad_norm = 0
        zero_grad_params = []

        for name, param in model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                total_grad_norm += grad_norm

                if grad_norm < 1e-6:
                    zero_grad_params.append((name, grad_norm))
                elif torch.isnan(param.grad).any():
                    print(f"  âŒ {name}: æ¢¯åº¦åŒ…å«NaN!")
            else:
                print(f"  âš ï¸ {name}: æ¢¯åº¦ä¸ºNone")

        print(f"  æ€»æ¢¯åº¦èŒƒæ•°: {total_grad_norm:.6f}")

        if zero_grad_params:
            print(f"  é›¶æ¢¯åº¦å‚æ•° ({len(zero_grad_params)}ä¸ª):")
            for name, norm in zero_grad_params[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"    {name}: {norm:.6e}")
            if len(zero_grad_params) > 5:
                print(f"    ... è¿˜æœ‰ {len(zero_grad_params) - 5} ä¸ª")

    @staticmethod
    def check_attention_maps(attention_maps):
        """æ£€æŸ¥æ³¨æ„åŠ›å›¾"""
        if attention_maps is None:
            print("  æ³¨æ„åŠ›å›¾ä¸ºNone")
            return

        print(f"\nğŸ” æ£€æŸ¥æ³¨æ„åŠ›å›¾:")
        print(f"  å½¢çŠ¶: {attention_maps.shape}")
        print(f"  æ•°å€¼èŒƒå›´: [{attention_maps.min():.6f}, {attention_maps.max():.6f}]")

        # æ£€æŸ¥æ¯ä¸ªåŒºåŸŸçš„æ³¨æ„åŠ›æ€»å’Œ
        if attention_maps.dim() == 4:
            print(f"  æ¯ä¸ªåƒç´ çš„æ³¨æ„åŠ›å€¼æ€»å’Œ:")
            for i in range(attention_maps.shape[1]):
                region_sum = attention_maps[:, i:i + 1].sum(dim=1).mean().item()
                print(f"    åŒºåŸŸ{i + 1}: {region_sum:.6f}")

    @staticmethod
    def check_model_parameters(model):
        """æ£€æŸ¥æ¨¡å‹å‚æ•°"""
        print(f"\nğŸ” æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  å†»ç»“å‚æ•°: {total_params - trainable_params:,}")

        return total_params, trainable_params

    @staticmethod
    def test_forward_pass(model, input_shape=(2, 3, 224, 224), device='cpu'):
        """æµ‹è¯•å‰å‘ä¼ æ’­"""
        print(f"\nğŸ” æµ‹è¯•å‰å‘ä¼ æ’­:")

        dummy_input = torch.randn(*input_shape).to(device)
        model.eval()

        try:
            with torch.no_grad():
                outputs = model(dummy_input)

            print(f"  âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")

            if isinstance(outputs, dict):
                for key, value in outputs.items():
                    if torch.is_tensor(value):
                        print(f"    {key}: {value.shape}")
                    elif value is not None:
                        print(f"    {key}: {type(value).__name__}")

            return outputs

        except Exception as e:
            print(f"  âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            return None


def test_basic_functionality(device):
    """æµ‹è¯•æ¨¡å‹åŸºæœ¬åŠŸèƒ½"""
    print("=" * 60)
    print("ğŸ§ª åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)

    debugger = ModelDebugger()

    # æµ‹è¯•è¾“å…¥
    dummy_input = torch.randn(2, 3, 224, 224).to(device)
    print(f"\nğŸ“Š æµ‹è¯•è¾“å…¥:")
    print(f"  å½¢çŠ¶: {dummy_input.shape}")
    print(f"  èŒƒå›´: [{dummy_input.min():.3f}, {dummy_input.max():.3f}]")

    # åˆ›å»ºç®€å•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = nn.Conv2d(3, 64, kernel_size=3, padding=1)
            self.pool = nn.MaxPool2d(2)
            self.fc = nn.Linear(64 * 112 * 112, 10)

        def forward(self, x):
            x = self.conv(x)
            x = self.pool(x)
            x = x.view(x.size(0), -1)
            x = self.fc(x)
            return x

    model = SimpleModel().to(device)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    print(f"\nğŸ”§ æµ‹è¯•ç®€å•æ¨¡å‹:")
    debugger.test_forward_pass(model, input_shape=(2, 3, 224, 224), device=device)

    # æµ‹è¯•æ¢¯åº¦
    print(f"\nğŸ“ˆ æµ‹è¯•æ¢¯åº¦è®¡ç®—:")
    model.train()
    output = model(dummy_input)
    dummy_labels = torch.randint(0, 10, (2,)).to(device)
    criterion = nn.CrossEntropyLoss()
    loss = criterion(output, dummy_labels)
    loss.backward()
    debugger.check_gradient(model, loss)

    print("\n" + "=" * 60)
    print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•å®Œæˆ")
    print("=" * 60)

    return True


# æ·»åŠ ä¸€ä¸ªæ–°çš„æ£€æŸ¥å‡½æ•°åˆ°debug_tools.py
def check_pretrained_model(model_path):
    """æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹"""
    print(f"\nğŸ” æ£€æŸ¥é¢„è®­ç»ƒæ¨¡å‹: {model_path}")

    if not os.path.exists(model_path):
        print("âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        return None

    try:
        checkpoint = torch.load(model_path, map_location='cpu')

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        print(f"æ–‡ä»¶å¤§å°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB")

        if 'model_state_dict' in checkpoint:
            state_dict = checkpoint['model_state_dict']
            print(f"å‚æ•°æ•°é‡: {len(state_dict)}")

            # æ‰“å°å‰å‡ ä¸ªé”®
            print("\nå‚æ•°é”®ç¤ºä¾‹:")
            for i, key in enumerate(list(state_dict.keys())[:10]):
                print(f"  {i + 1}. {key}: {state_dict[key].shape}")

            # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†ç±»å™¨
            classifier_keys = [k for k in state_dict.keys() if 'classifier' in k or 'fc' in k]
            if classifier_keys:
                print(f"\nåˆ†ç±»å™¨å±‚ ({len(classifier_keys)}ä¸ª):")
                for key in classifier_keys:
                    print(f"  {key}: {state_dict[key].shape}")

        if 'config' in checkpoint:
            print(f"\næ¨¡å‹é…ç½®:")
            for key, value in checkpoint['config'].items():
                print(f"  {key}: {value}")

        if 'best_test_acc' in checkpoint:
            print(f"\nå†å²æœ€ä½³å‡†ç¡®ç‡: {checkpoint['best_test_acc']:.4f}")

        return checkpoint

    except Exception as e:
        print(f"âŒ æ£€æŸ¥å¤±è´¥: {e}")
        return None