import os
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from tqdm import tqdm
import gc

from config import *
from model_components import ImprovedDualPathModel


def train_model(device, model_path=MODEL_PATH, metrics_path=METRICS_PATH):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"\nğŸš€ å¼€å§‹è®­ç»ƒåŒè·¯å¾„æ¨¡å‹...")

    # æ¸…ç†ç¼“å­˜
    torch.cuda.empty_cache()
    gc.collect()

    # åŠ è½½æ•°æ®
    from data_loader import create_data_loaders
    train_loader, test_loader, train_dataset, test_dataset = create_data_loaders()

    print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

    # æ„å»ºæ¨¡å‹
    print(f"\nğŸ”§ æ„å»ºæ¨¡å‹...")
    model = ImprovedDualPathModel(
        num_classes=NUM_CLASSES,
        num_regions=NUM_REGIONS
    )
    model = model.to(device)

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"å†»ç»“å‚æ•°é‡: {total_params - trainable_params:,}")

    # ä¼˜åŒ–å™¨
    optimizer = optim.Adam(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=0.0001,
        weight_decay=1e-5
    )
    # warmup_epochs = 3  # é¢„çƒ­1ä¸ªepochï¼ˆçº¦63ä¸ªbatchï¼‰
    # warmup_factor = 10.0  # é¢„çƒ­æœŸå­¦ä¹ ç‡æ”¾å¤§10å€ï¼ˆ1e-3 â†’ 1e-2ï¼‰

    # def warmup_lr_scheduler(epoch, batch_idx, total_batches):
    #     """åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡"""
    #     total_warmup_steps = warmup_epochs * total_batches  # æ€»å…±çš„é¢„çƒ­æ­¥æ•°
    #     current_step = epoch * total_batches + batch_idx
    #
    #     if current_step < total_warmup_steps:
    #         # çº¿æ€§é¢„çƒ­ï¼šä»0åˆ°1e-2
    #         alpha = current_step / total_warmup_steps
    #         warmup_lr = LEARNING_RATE * warmup_factor * alpha
    #         for param_group in optimizer.param_groups:
    #             param_group['lr'] = warmup_lr
    #     else:
            # é¢„çƒ­ç»“æŸï¼Œä½¿ç”¨æ­£å¸¸å­¦ä¹ ç‡
            # for param_group in optimizer.param_groups:
            #     param_group['lr'] = LEARNING_RATE



    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.93)

    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()

    # è®­ç»ƒæŒ‡æ ‡
    train_losses = []
    train_accs = []
    test_losses = []
    test_accs = []
    best_test_acc = 0.0

    print(f"\nğŸ“ˆ å¼€å§‹è®­ç»ƒï¼Œå…± {EPOCHS} ä¸ªepoch...")

    for epoch in range(EPOCHS):
        # ===== è®­ç»ƒé˜¶æ®µ =====
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        fusion_stats = []

        pbar = tqdm(train_loader, desc=f"Epoch {epoch + 1}/{EPOCHS} [Train]")
        for batch_idx, (images, labels) in enumerate(pbar):
            # å®šæœŸæ¸…ç†ç¼“å­˜
            if batch_idx % 20 == 0:
                torch.cuda.empty_cache()

            images, labels = images.to(device), labels.to(device)

            # å‰å‘ä¼ æ’­
            outputs = model(images)

            # è®¡ç®—æŸå¤±
            loss, loss_details = model.compute_loss(outputs, labels, criterion)

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()

            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(
                filter(lambda p: p.requires_grad, model.parameters()),
                max_norm=0.5
            )

            optimizer.step()

            # ç»Ÿè®¡
            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs['final_logits'], 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # æ”¶é›†èåˆæƒé‡
            if 'fusion_weights' in outputs:
                fusion_stats.append(outputs['fusion_weights'].mean(dim=0).cpu().detach().numpy())

            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                "loss": running_loss / max(total, 1),
                "acc": correct / max(total, 1),
                **{k: v for k, v in loss_details.items() if 'loss' in k}
            })

        train_loss = running_loss / len(train_loader.dataset)
        train_acc = correct / total

        # ===== éªŒè¯é˜¶æ®µ =====
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0

        with torch.no_grad():
            pbar = tqdm(test_loader, desc=f"Epoch {epoch + 1}/{EPOCHS} [Val]")
            for images, labels in pbar:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)

                loss = criterion(outputs['final_logits'], labels)

                running_loss += loss.item() * images.size(0)
                _, predicted = torch.max(outputs['final_logits'], 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

                pbar.set_postfix({
                    "loss": running_loss / max(total, 1),
                    "acc": correct / max(total, 1)
                })

        test_loss = running_loss / len(test_loader.dataset)
        test_acc = correct / total

        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step()

        # è®°å½•æŒ‡æ ‡
        train_losses.append(train_loss)
        train_accs.append(train_acc)
        test_losses.append(test_loss)
        test_accs.append(test_acc)

        # æ‰“å°ç»“æœ
        print(f"\nğŸ“Š Epoch {epoch + 1} ç»“æœ:")
        print(f"  è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.4f}")
        print(f"  æµ‹è¯•æŸå¤±: {test_loss:.4f}, æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

        # ä¿å­˜æœ€ä¼˜æ¨¡å‹
        if test_acc > best_test_acc:
            best_test_acc = test_acc
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'best_test_acc': best_test_acc,
                'config': {
                    'model_type': 'improved_dual_path',
                    'num_regions': NUM_REGIONS,
                    'image_size': IMAGE_SIZE,
                    'batch_size': BATCH_SIZE,
                    'num_classes': NUM_CLASSES
                }
            }, model_path)
            print(f"  ğŸ’¾ ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼Œå‡†ç¡®ç‡: {best_test_acc:.4f}")

    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = {
        'train_losses': train_losses,
        'train_accs': train_accs,
        'test_losses': test_losses,
        'test_accs': test_accs,
        'best_test_acc': best_test_acc,
        'epochs': EPOCHS
    }
    np.save(metrics_path, metrics)

    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {best_test_acc:.4f}")

    return model, train_losses, train_accs, test_losses, test_accs


def test_model(device, model_path=MODEL_PATH):
    """æµ‹è¯•æ¨¡å‹"""
    if not os.path.exists(model_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè®­ç»ƒæ¨¡å‹ï¼")
        return

    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹...")

    # åŠ è½½æ•°æ®
    from data_loader import create_data_loaders
    _, test_loader, _, test_dataset = create_data_loaders()

    # æ„å»ºæ¨¡å‹
    model = ImprovedDualPathModel(
        num_classes=NUM_CLASSES,
        num_regions=NUM_REGIONS
    )
    model = model.to(device)

    # åŠ è½½æƒé‡
    checkpoint = torch.load(model_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()

    # æµ‹è¯•è¯„ä¼°
    criterion = nn.CrossEntropyLoss()
    running_loss = 0.0
    correct = 0
    total = 0

    print(f"å¼€å§‹æµ‹è¯•...")

    with torch.no_grad():
        pbar = tqdm(test_loader, desc="æµ‹è¯•ä¸­")
        for images, labels in pbar:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)

            # è®¡ç®—æŸå¤±
            loss = criterion(outputs['final_logits'], labels)

            # ç»Ÿè®¡æŒ‡æ ‡
            running_loss += loss.item() * images.size(0)
            _, predicted = torch.max(outputs['final_logits'], 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            pbar.set_postfix({
                "acc": correct / max(total, 1)
            })

    # è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    final_loss = running_loss / len(test_loader.dataset)
    final_acc = correct / total

    print(f"\nğŸ“ˆ æµ‹è¯•ç»“æœ:")
    print(f"æµ‹è¯•æŸå¤±: {final_loss:.4f}")
    print(f"æµ‹è¯•å‡†ç¡®ç‡: {final_acc:.4f}")
    print(f"æœ€ä½³å†å²å‡†ç¡®ç‡: {checkpoint.get('best_test_acc', 0.0):.4f}")

    return final_acc